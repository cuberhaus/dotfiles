//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32603126
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.1
.target sm_61, texmode_independent
.address_size 64

	// .globl	DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power

.entry DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power(
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_0,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_1,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_2,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_3,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_4,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_5,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_6,
	.param .f64 DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_7
)
{
	.reg .pred 	%p<129>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<78>;
	.reg .f64 	%fd<339>;
	.reg .b64 	%rd<40>;


	ld.param.u64 	%rd13, [DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_6];
	mov.b32 	%r16, %envreg3;
	mov.u32 	%r17, %ctaid.x;
	mov.u32 	%r18, %ntid.x;
	mov.u32 	%r19, %tid.x;
	add.s32 	%r20, %r19, %r16;
	mad.lo.s32 	%r21, %r18, %r17, %r20;
	cvt.s64.s32 	%rd1, %r21;
	setp.gt.s32 	%p1, %r21, 4;
	mov.f64 	%fd316, 0d7FFFFFFFE0000000;
	@%p1 bra 	$L__BB0_2;

	shl.b64 	%rd14, %rd1, 3;
	add.s64 	%rd15, %rd13, %rd14;
	ld.global.f64 	%fd316, [%rd15];

$L__BB0_2:
	abs.f64 	%fd75, %fd316;
	setp.gtu.f64 	%p2, %fd75, 0d7FF0000000000000;
	cvt.u32.u64 	%r22, %rd1;
	setp.gt.s32 	%p3, %r22, 4;
	or.pred  	%p4, %p3, %p2;
	selp.f64 	%fd3, 0d0000000000000000, %fd316, %p4;
	setp.eq.f64 	%p5, %fd3, 0d3FF0000000000000;
	mov.f64 	%fd320, 0d3FF0000000000000;
	@%p5 bra 	$L__BB0_28;

	abs.f64 	%fd4, %fd3;
	setp.gtu.f64 	%p6, %fd4, 0d7FF0000000000000;
	@%p6 bra 	$L__BB0_27;
	bra.uni 	$L__BB0_4;

$L__BB0_27:
	add.f64 	%fd320, %fd3, 0d3FFCAC083126E979;
	bra.uni 	$L__BB0_28;

$L__BB0_4:
	setp.eq.f64 	%p7, %fd3, 0d7FF0000000000000;
	@%p7 bra 	$L__BB0_26;
	bra.uni 	$L__BB0_5;

$L__BB0_26:
	mov.f64 	%fd263, 0d3FFCAC083126E979;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r53}, %fd263;
	}
	setp.gt.s32 	%p30, %r53, -1;
	selp.f64 	%fd320, 0d7FF0000000000000, 0d0000000000000000, %p30;

$L__BB0_28:
	ld.param.u64 	%rd34, [DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_5];
	add.f64 	%fd23, %fd320, 0d0000000000000000;
	shl.b64 	%rd18, %rd1, 3;
	add.s64 	%rd2, %rd34, %rd18;
	mov.f64 	%fd321, 0d7FFFFFFFE0000000;
	@%p3 bra 	$L__BB0_30;

	ld.global.f64 	%fd321, [%rd2];

$L__BB0_30:
	abs.f64 	%fd266, %fd321;
	setp.gtu.f64 	%p32, %fd266, 0d7FF0000000000000;
	mov.f64 	%fd330, 0d0000000000000000;
	@%p32 bra 	$L__BB0_56;

	mov.f64 	%fd323, 0d7FFFFFFFE0000000;
	mov.f64 	%fd322, %fd323;
	@%p3 bra 	$L__BB0_33;

	ld.global.f64 	%fd322, [%rd2];

$L__BB0_33:
	ld.param.u64 	%rd35, [DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_4];
	add.s64 	%rd3, %rd35, %rd18;
	@%p3 bra 	$L__BB0_35;

	ld.global.f64 	%fd323, [%rd3];

$L__BB0_35:
	abs.f64 	%fd270, %fd323;
	setp.gtu.f64 	%p35, %fd270, 0d7FF0000000000000;
	mov.f64 	%fd269, 0d7FF8000000000214;
	mov.f64 	%fd329, %fd269;
	@%p35 bra 	$L__BB0_55;

	mov.f64 	%fd325, 0d7FFFFFFFE0000000;
	mov.f64 	%fd324, %fd325;
	@%p3 bra 	$L__BB0_38;

	ld.global.f64 	%fd324, [%rd3];

$L__BB0_38:
	ld.param.u64 	%rd36, [DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_3];
	add.s64 	%rd4, %rd36, %rd18;
	@%p3 bra 	$L__BB0_40;

	ld.global.f64 	%fd325, [%rd4];

$L__BB0_40:
	abs.f64 	%fd273, %fd325;
	setp.le.f64 	%p38, %fd273, 0d7FF0000000000000;
	@%p38 bra 	$L__BB0_46;

	mov.f64 	%fd326, 0d7FFFFFFFE0000000;
	@%p3 bra 	$L__BB0_43;

	ld.global.f64 	%fd326, [%rd3];

$L__BB0_43:
	abs.f64 	%fd275, %fd326;
	setp.gtu.f64 	%p40, %fd275, 0d7FF0000000000000;
	mov.f64 	%fd329, 0d0000000000000000;
	@%p40 bra 	$L__BB0_46;

	@%p3 bra 	$L__BB0_55;

	ld.global.f64 	%fd278, [%rd3];
	setp.neu.f64 	%p42, %fd278, 0d0000000000000000;
	@%p42 bra 	$L__BB0_55;

$L__BB0_46:
	mov.f64 	%fd327, 0d7FFFFFFFE0000000;
	@%p3 bra 	$L__BB0_48;

	ld.global.f64 	%fd327, [%rd4];

$L__BB0_48:
	abs.f64 	%fd280, %fd327;
	setp.gtu.f64 	%p44, %fd280, 0d7FF0000000000000;
	@%p44 bra 	$L__BB0_53;
	bra.uni 	$L__BB0_49;

$L__BB0_53:
	setp.eq.f64 	%p47, %fd324, 0d0000000000000000;
	mov.f64 	%fd329, %fd269;
	@%p47 bra 	$L__BB0_55;

	rcp.rn.f64 	%fd329, %fd324;
	bra.uni 	$L__BB0_55;

$L__BB0_49:
	setp.eq.f64 	%p45, %fd324, 0d0000000000000000;
	mov.f64 	%fd329, %fd269;
	@%p45 bra 	$L__BB0_55;

	mov.f64 	%fd328, 0d7FFFFFFFE0000000;
	@%p3 bra 	$L__BB0_52;

	ld.global.f64 	%fd328, [%rd4];

$L__BB0_52:
	div.rn.f64 	%fd329, %fd328, %fd324;

$L__BB0_55:
	mul.f64 	%fd330, %fd322, %fd329;

$L__BB0_56:
	ld.param.u64 	%rd37, [DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_2];
	add.s64 	%rd5, %rd37, %rd18;
	mov.f64 	%fd331, 0d7FFFFFFFE0000000;
	@%p3 bra 	$L__BB0_58;

	ld.global.f64 	%fd331, [%rd5];

$L__BB0_58:
	abs.f64 	%fd286, %fd331;
	setp.gtu.f64 	%p49, %fd286, 0d7FF0000000000000;
	mov.f64 	%fd333, 0d0000000000000000;
	@%p49 bra 	$L__BB0_62;

	mov.f64 	%fd332, 0d7FFFFFFFE0000000;
	@%p3 bra 	$L__BB0_61;

	ld.global.f64 	%fd332, [%rd5];

$L__BB0_61:
	add.f64 	%fd333, %fd332, 0d0000000000000000;

$L__BB0_62:
	ld.param.u64 	%rd38, [DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_1];
	add.s64 	%rd6, %rd38, %rd18;
	mov.f64 	%fd334, 0d7FFFFFFFE0000000;
	@%p3 bra 	$L__BB0_64;

	ld.global.f64 	%fd334, [%rd6];

$L__BB0_64:
	abs.f64 	%fd289, %fd334;
	setp.gtu.f64 	%p52, %fd289, 0d7FF0000000000000;
	@%p52 bra 	$L__BB0_79;
	bra.uni 	$L__BB0_65;

$L__BB0_79:
	add.f64 	%fd336, %fd333, 0d0000000000000000;
	bra.uni 	$L__BB0_80;

$L__BB0_65:
	mov.f64 	%fd335, 0d7FFFFFFFE0000000;
	@%p3 bra 	$L__BB0_67;

	ld.global.f64 	%fd335, [%rd6];

$L__BB0_67:
	setp.gt.f64 	%p54, %fd333, 0d0000000000000000;
	setp.lt.f64 	%p55, %fd335, 0d0000000000000000;
	and.pred  	%p56, %p54, %p55;
	@%p56 bra 	$L__BB0_69;

	setp.geu.f64 	%p57, %fd333, 0d0000000000000000;
	setp.leu.f64 	%p58, %fd335, 0d0000000000000000;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB0_78;

$L__BB0_69:
	neg.f64 	%fd55, %fd333;
	setp.eq.f64 	%p60, %fd335, %fd55;
	mov.f64 	%fd336, 0d0000000000000000;
	@%p60 bra 	$L__BB0_80;

	setp.eq.f64 	%p61, %fd335, 0d0000000000000000;
	setp.eq.f64 	%p62, %fd333, 0d8000000000000000;
	or.pred  	%p63, %p62, %p61;
	@%p63 bra 	$L__BB0_78;

	add.f64 	%fd292, %fd333, %fd335;
	abs.f64 	%fd56, %fd292;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r67}, %fd56;
	}
	and.b32  	%r68, %r67, 2146435072;
	setp.eq.s32 	%p64, %r68, 2146435072;
	@%p64 bra 	$L__BB0_78;

	abs.f64 	%fd57, %fd335;
	mul.f64 	%fd293, %fd57, 0d3D30000000000000;
	setp.gt.f64 	%p65, %fd56, %fd293;
	@%p65 bra 	$L__BB0_78;

	abs.f64 	%fd58, %fd55;
	mul.f64 	%fd294, %fd58, 0d3D30000000000000;
	setp.gt.f64 	%p66, %fd56, %fd294;
	@%p66 bra 	$L__BB0_78;

	setp.gtu.f64 	%p67, %fd56, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd23, %fd56;
	setp.gt.s64 	%p68, %rd23, 9007199254740991;
	or.pred  	%p69, %p67, %p68;
	@%p69 bra 	$L__BB0_77;

	setp.gtu.f64 	%p70, %fd57, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd24, %fd57;
	setp.gt.s64 	%p71, %rd24, 9007199254740991;
	or.pred  	%p72, %p70, %p71;
	@%p72 bra 	$L__BB0_77;

	setp.le.f64 	%p73, %fd58, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd25, %fd58;
	setp.lt.s64 	%p74, %rd25, 9007199254740992;
	and.pred  	%p75, %p73, %p74;
	@%p75 bra 	$L__BB0_78;

$L__BB0_77:
	mul.f64 	%fd296, %fd57, 0d3CF0000000000000;
	setp.lt.f64 	%p76, %fd56, %fd296;
	mul.f64 	%fd297, %fd58, 0d3CF0000000000000;
	setp.lt.f64 	%p77, %fd56, %fd297;
	and.pred  	%p78, %p76, %p77;
	@%p78 bra 	$L__BB0_80;

$L__BB0_78:
	add.f64 	%fd336, %fd333, %fd335;

$L__BB0_80:
	setp.lt.f64 	%p79, %fd330, 0d0000000000000000;
	setp.lt.f64 	%p80, %fd336, 0d0000000000000000;
	and.pred  	%p81, %p79, %p80;
	@%p81 bra 	$L__BB0_82;

	setp.leu.f64 	%p82, %fd336, 0d0000000000000000;
	setp.leu.f64 	%p83, %fd330, 0d0000000000000000;
	or.pred  	%p84, %p83, %p82;
	@%p84 bra 	$L__BB0_91;

$L__BB0_82:
	setp.eq.f64 	%p85, %fd336, %fd330;
	mov.f64 	%fd337, 0d0000000000000000;
	@%p85 bra 	$L__BB0_92;

	setp.eq.f64 	%p86, %fd336, 0d0000000000000000;
	setp.eq.f64 	%p87, %fd330, 0d0000000000000000;
	or.pred  	%p88, %p87, %p86;
	@%p88 bra 	$L__BB0_91;

	sub.f64 	%fd299, %fd336, %fd330;
	abs.f64 	%fd62, %fd299;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r69}, %fd62;
	}
	and.b32  	%r70, %r69, 2146435072;
	setp.eq.s32 	%p89, %r70, 2146435072;
	@%p89 bra 	$L__BB0_91;

	abs.f64 	%fd63, %fd336;
	mul.f64 	%fd300, %fd63, 0d3D30000000000000;
	setp.gt.f64 	%p90, %fd62, %fd300;
	@%p90 bra 	$L__BB0_91;

	abs.f64 	%fd64, %fd330;
	mul.f64 	%fd301, %fd64, 0d3D30000000000000;
	setp.gt.f64 	%p91, %fd62, %fd301;
	@%p91 bra 	$L__BB0_91;

	setp.gtu.f64 	%p92, %fd62, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd26, %fd62;
	setp.gt.s64 	%p93, %rd26, 9007199254740991;
	or.pred  	%p94, %p92, %p93;
	@%p94 bra 	$L__BB0_90;

	setp.gtu.f64 	%p95, %fd63, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd27, %fd63;
	setp.gt.s64 	%p96, %rd27, 9007199254740991;
	or.pred  	%p97, %p95, %p96;
	@%p97 bra 	$L__BB0_90;

	setp.le.f64 	%p98, %fd64, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd28, %fd64;
	setp.lt.s64 	%p99, %rd28, 9007199254740992;
	and.pred  	%p100, %p98, %p99;
	@%p100 bra 	$L__BB0_91;

$L__BB0_90:
	mul.f64 	%fd303, %fd63, 0d3CF0000000000000;
	setp.lt.f64 	%p101, %fd62, %fd303;
	mul.f64 	%fd304, %fd64, 0d3CF0000000000000;
	setp.lt.f64 	%p102, %fd62, %fd304;
	and.pred  	%p103, %p101, %p102;
	@%p103 bra 	$L__BB0_92;

$L__BB0_91:
	sub.f64 	%fd337, %fd336, %fd330;

$L__BB0_92:
	setp.gt.f64 	%p104, %fd23, 0d0000000000000000;
	setp.lt.f64 	%p105, %fd337, 0d0000000000000000;
	and.pred  	%p106, %p104, %p105;
	@%p106 bra 	$L__BB0_94;

	setp.geu.f64 	%p107, %fd23, 0d0000000000000000;
	setp.leu.f64 	%p108, %fd337, 0d0000000000000000;
	or.pred  	%p109, %p107, %p108;
	@%p109 bra 	$L__BB0_103;

$L__BB0_94:
	neg.f64 	%fd67, %fd23;
	setp.eq.f64 	%p110, %fd337, %fd67;
	mov.f64 	%fd338, 0d0000000000000000;
	@%p110 bra 	$L__BB0_104;

	setp.eq.f64 	%p111, %fd337, 0d0000000000000000;
	setp.eq.f64 	%p112, %fd23, 0d8000000000000000;
	or.pred  	%p113, %p112, %p111;
	@%p113 bra 	$L__BB0_103;

	add.f64 	%fd306, %fd23, %fd337;
	abs.f64 	%fd68, %fd306;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r71}, %fd68;
	}
	and.b32  	%r72, %r71, 2146435072;
	setp.eq.s32 	%p114, %r72, 2146435072;
	@%p114 bra 	$L__BB0_103;

	abs.f64 	%fd69, %fd337;
	mul.f64 	%fd307, %fd69, 0d3D30000000000000;
	setp.gt.f64 	%p115, %fd68, %fd307;
	@%p115 bra 	$L__BB0_103;

	abs.f64 	%fd70, %fd67;
	mul.f64 	%fd308, %fd70, 0d3D30000000000000;
	setp.gt.f64 	%p116, %fd68, %fd308;
	@%p116 bra 	$L__BB0_103;

	setp.gtu.f64 	%p117, %fd68, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd29, %fd68;
	setp.gt.s64 	%p118, %rd29, 9007199254740991;
	or.pred  	%p119, %p117, %p118;
	@%p119 bra 	$L__BB0_102;

	setp.gtu.f64 	%p120, %fd69, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd30, %fd69;
	setp.gt.s64 	%p121, %rd30, 9007199254740991;
	or.pred  	%p122, %p120, %p121;
	@%p122 bra 	$L__BB0_102;

	setp.le.f64 	%p123, %fd70, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd31, %fd70;
	setp.lt.s64 	%p124, %rd31, 9007199254740992;
	and.pred  	%p125, %p123, %p124;
	@%p125 bra 	$L__BB0_103;

$L__BB0_102:
	mul.f64 	%fd310, %fd69, 0d3CF0000000000000;
	setp.lt.f64 	%p126, %fd68, %fd310;
	mul.f64 	%fd311, %fd70, 0d3CF0000000000000;
	setp.lt.f64 	%p127, %fd68, %fd311;
	and.pred  	%p128, %p126, %p127;
	@%p128 bra 	$L__BB0_104;

$L__BB0_103:
	add.f64 	%fd338, %fd23, %fd337;

$L__BB0_104:
	ld.param.u64 	%rd39, [DynamicKernel_nop_fsum_fsub_fsum_fmul_fdiv_Power_param_0];
	add.s64 	%rd33, %rd39, %rd18;
	st.global.f64 	[%rd33], %fd338;
	ret;

$L__BB0_5:
	mov.f64 	%fd76, 0d3FFCAC083126E979;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r23, %temp}, %fd76;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r24}, %fd76;
	}
	and.b32  	%r25, %r24, 2147483647;
	setp.ne.s32 	%p8, %r25, 2146435072;
	setp.ne.s32 	%p9, %r23, 0;
	or.pred  	%p10, %p9, %p8;
	@%p10 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_6;

$L__BB0_8:
	mov.f64 	%fd79, 0d3FE0000000000000;
	mul.rn.f64 	%fd80, %fd79, %fd76;
	cvt.rzi.f64.f64 	%fd81, %fd80;
	mov.f64 	%fd82, 0d4000000000000000;
	mul.rn.f64 	%fd83, %fd82, %fd81;
	sub.f64 	%fd84, %fd76, %fd83;
	abs.f64 	%fd6, %fd84;
	setp.eq.f64 	%p13, %fd3, 0d0000000000000000;
	@%p13 bra 	$L__BB0_25;
	bra.uni 	$L__BB0_9;

$L__BB0_25:
	setp.eq.f64 	%p29, %fd6, 0d3FF0000000000000;
	selp.f64 	%fd320, %fd3, 0d0000000000000000, %p29;
	bra.uni 	$L__BB0_28;

$L__BB0_6:
	setp.eq.f64 	%p11, %fd3, 0dBFF0000000000000;
	@%p11 bra 	$L__BB0_28;

	setp.gt.f64 	%p12, %fd4, 0d3FF0000000000000;
	selp.f64 	%fd320, 0d7FF0000000000000, 0d0000000000000000, %p12;
	bra.uni 	$L__BB0_28;

$L__BB0_9:
	setp.eq.f64 	%p14, %fd3, 0dFFF0000000000000;
	@%p14 bra 	$L__BB0_23;
	bra.uni 	$L__BB0_10;

$L__BB0_23:
	setp.neu.f64 	%p28, %fd6, 0d3FF0000000000000;
	mov.f64 	%fd320, 0d7FF0000000000000;
	@%p28 bra 	$L__BB0_28;

	mov.f64 	%fd320, 0dFFF0000000000000;
	bra.uni 	$L__BB0_28;

$L__BB0_10:
	setp.geu.f64 	%p15, %fd3, 0d0000000000000000;
	@%p15 bra 	$L__BB0_12;

	mov.f64 	%fd86, 0d3FFCAC083126E979;
	cvt.rzi.f64.f64 	%fd87, %fd86;
	setp.neu.f64 	%p16, %fd87, 0d3FFCAC083126E979;
	mov.f64 	%fd320, 0dFFF8000000000000;
	@%p16 bra 	$L__BB0_28;

$L__BB0_12:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r75}, %fd4; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r74, hi}, %fd4; 
	}
	// end inline asm
	bfe.u32 	%r76, %r75, 20, 11;
	setp.ne.s32 	%p17, %r76, 0;
	@%p17 bra 	$L__BB0_14;

	mov.f64 	%fd92, 0d4350000000000000;
	mul.rn.f64 	%fd91, %fd4, %fd92;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r75}, %fd91; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r74, hi}, %fd91; 
	}
	// end inline asm
	bfe.u32 	%r30, %r75, 20, 11;
	add.s32 	%r76, %r30, -54;

$L__BB0_14:
	add.s32 	%r77, %r76, -1023;
	and.b32  	%r33, %r75, -2146435073;
	or.b32  	%r32, %r33, 1072693248;
	// begin inline asm
	mov.b64 	%fd317, {%r74, %r32};
	// end inline asm
	setp.lt.u32 	%p18, %r32, 1073127583;
	@%p18 bra 	$L__BB0_16;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r34, hi}, %fd317; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r35}, %fd317; 
	}
	// end inline asm
	add.s32 	%r37, %r35, -1048576;
	// begin inline asm
	mov.b64 	%fd317, {%r34, %r37};
	// end inline asm
	add.s32 	%r77, %r76, -1022;

$L__BB0_16:
	add.f64 	%fd181, %fd317, 0d3FF0000000000000;
	mov.f64 	%fd182, 0d3FF0000000000000;
	rcp.rn.f64 	%fd183, %fd181;
	add.f64 	%fd123, %fd317, 0dBFF0000000000000;
	mul.rn.f64 	%fd184, %fd123, %fd183;
	add.f64 	%fd171, %fd184, %fd184;
	mul.rn.f64 	%fd119, %fd171, %fd171;
	mov.f64 	%fd98, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd100, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd97, %fd98, %fd119, %fd100;
	// end inline asm
	mov.f64 	%fd104, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd101, %fd97, %fd119, %fd104;
	// end inline asm
	mov.f64 	%fd108, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd105, %fd101, %fd119, %fd108;
	// end inline asm
	mov.f64 	%fd112, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd109, %fd105, %fd119, %fd112;
	// end inline asm
	mov.f64 	%fd116, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd113, %fd109, %fd119, %fd116;
	// end inline asm
	mov.f64 	%fd120, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd117, %fd113, %fd119, %fd120;
	// end inline asm
	mul.rn.f64 	%fd185, %fd117, %fd119;
	sub.f64 	%fd186, %fd123, %fd171;
	mov.f64 	%fd187, 0d4000000000000000;
	mul.rn.f64 	%fd124, %fd187, %fd186;
	neg.f64 	%fd122, %fd171;
	// begin inline asm
	fma.rn.f64 	%fd121, %fd122, %fd123, %fd124;
	// end inline asm
	mul.rn.f64 	%fd167, %fd183, %fd121;
	add.f64 	%fd188, %fd185, 0d3FB5555555555555;
	mov.f64 	%fd189, 0d3FB5555555555555;
	sub.f64 	%fd190, %fd189, %fd188;
	add.f64 	%fd191, %fd185, %fd190;
	add.f64 	%fd192, %fd191, 0d0000000000000000;
	add.f64 	%fd193, %fd192, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd134, %fd188, %fd193;
	sub.f64 	%fd194, %fd188, %fd134;
	add.f64 	%fd138, %fd193, %fd194;
	mul.rn.f64 	%fd195, %fd134, %fd171;
	neg.f64 	%fd128, %fd195;
	// begin inline asm
	fma.rn.f64 	%fd125, %fd134, %fd171, %fd128;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd129, %fd138, %fd167, %fd125;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd133, %fd134, %fd167, %fd129;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd137, %fd138, %fd171, %fd133;
	// end inline asm
	add.f64 	%fd150, %fd195, %fd137;
	sub.f64 	%fd196, %fd195, %fd150;
	add.f64 	%fd154, %fd137, %fd196;
	mul.rn.f64 	%fd197, %fd150, %fd171;
	neg.f64 	%fd144, %fd197;
	// begin inline asm
	fma.rn.f64 	%fd141, %fd150, %fd171, %fd144;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd145, %fd154, %fd167, %fd141;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd149, %fd150, %fd167, %fd145;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd153, %fd154, %fd171, %fd149;
	// end inline asm
	add.f64 	%fd166, %fd197, %fd153;
	sub.f64 	%fd198, %fd197, %fd166;
	add.f64 	%fd170, %fd153, %fd198;
	mul.rn.f64 	%fd199, %fd166, %fd171;
	neg.f64 	%fd160, %fd199;
	// begin inline asm
	fma.rn.f64 	%fd157, %fd166, %fd171, %fd160;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd161, %fd170, %fd167, %fd157;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd165, %fd166, %fd167, %fd161;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd169, %fd170, %fd171, %fd165;
	// end inline asm
	add.f64 	%fd200, %fd199, %fd169;
	sub.f64 	%fd201, %fd199, %fd200;
	add.f64 	%fd202, %fd169, %fd201;
	add.f64 	%fd203, %fd171, %fd200;
	sub.f64 	%fd204, %fd171, %fd203;
	add.f64 	%fd205, %fd200, %fd204;
	add.f64 	%fd206, %fd202, %fd205;
	add.f64 	%fd207, %fd167, %fd206;
	add.f64 	%fd208, %fd203, %fd207;
	sub.f64 	%fd209, %fd203, %fd208;
	add.f64 	%fd210, %fd207, %fd209;
	cvt.rn.f64.s32 	%fd211, %r77;
	mov.f64 	%fd212, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd213, %fd211, %fd212;
	mov.f64 	%fd214, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd215, %fd211, %fd214;
	add.f64 	%fd216, %fd213, %fd208;
	sub.f64 	%fd217, %fd213, %fd216;
	add.f64 	%fd218, %fd208, %fd217;
	add.f64 	%fd219, %fd210, %fd218;
	add.f64 	%fd220, %fd215, %fd219;
	add.f64 	%fd174, %fd216, %fd220;
	sub.f64 	%fd221, %fd216, %fd174;
	add.f64 	%fd178, %fd220, %fd221;
	mov.f64 	%fd179, 0d3FFCAC083126E979;
	mul.rn.f64 	%fd222, %fd174, %fd179;
	neg.f64 	%fd176, %fd222;
	// begin inline asm
	fma.rn.f64 	%fd173, %fd174, %fd179, %fd176;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd177, %fd178, %fd179, %fd173;
	// end inline asm
	add.f64 	%fd10, %fd222, %fd177;
	sub.f64 	%fd223, %fd222, %fd10;
	add.f64 	%fd11, %fd177, %fd223;
	mov.f64 	%fd224, 0d4338000000000000;
	mov.f64 	%fd225, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd226, %fd10, %fd225, %fd224;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd226;
	}
	mov.f64 	%fd227, 0dC338000000000000;
	add.rn.f64 	%fd228, %fd226, %fd227;
	mov.f64 	%fd229, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd230, %fd228, %fd229, %fd10;
	mov.f64 	%fd231, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd232, %fd228, %fd231, %fd230;
	mov.f64 	%fd233, 0d3E928AF3FCA213EA;
	mov.f64 	%fd234, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd235, %fd234, %fd232, %fd233;
	mov.f64 	%fd236, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd237, %fd235, %fd232, %fd236;
	mov.f64 	%fd238, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd239, %fd237, %fd232, %fd238;
	mov.f64 	%fd240, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd241, %fd239, %fd232, %fd240;
	mov.f64 	%fd242, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd243, %fd241, %fd232, %fd242;
	mov.f64 	%fd244, 0d3F81111111122322;
	fma.rn.f64 	%fd245, %fd243, %fd232, %fd244;
	mov.f64 	%fd246, 0d3FA55555555502A1;
	fma.rn.f64 	%fd247, %fd245, %fd232, %fd246;
	mov.f64 	%fd248, 0d3FC5555555555511;
	fma.rn.f64 	%fd249, %fd247, %fd232, %fd248;
	mov.f64 	%fd250, 0d3FE000000000000B;
	fma.rn.f64 	%fd251, %fd249, %fd232, %fd250;
	fma.rn.f64 	%fd252, %fd251, %fd232, %fd182;
	fma.rn.f64 	%fd253, %fd252, %fd232, %fd182;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd253;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd253;
	}
	shl.b32 	%r38, %r13, 20;
	add.s32 	%r39, %r15, %r38;
	mov.b64 	%fd320, {%r14, %r39};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd10;
	}
	mov.b32 	%f2, %r40;
	abs.f32 	%f1, %f2;
	setp.lt.f32 	%p19, %f1, 0f4086232B;
	@%p19 bra 	$L__BB0_19;

	setp.lt.f64 	%p20, %fd10, 0d0000000000000000;
	add.f64 	%fd254, %fd10, 0d7FF0000000000000;
	selp.f64 	%fd320, 0d0000000000000000, %fd254, %p20;
	setp.geu.f32 	%p21, %f1, 0f40874800;
	@%p21 bra 	$L__BB0_19;

	mov.f64 	%fd315, 0d4338000000000000;
	mov.f64 	%fd314, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd313, %fd10, %fd314, %fd315;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r73, %temp}, %fd313;
	}
	shr.u32 	%r41, %r73, 31;
	add.s32 	%r42, %r73, %r41;
	shr.s32 	%r43, %r42, 1;
	shl.b32 	%r44, %r43, 20;
	add.s32 	%r45, %r15, %r44;
	mov.b64 	%fd255, {%r14, %r45};
	sub.s32 	%r46, %r73, %r43;
	shl.b32 	%r47, %r46, 20;
	add.s32 	%r48, %r47, 1072693248;
	mov.u32 	%r49, 0;
	mov.b64 	%fd256, {%r49, %r48};
	mul.f64 	%fd320, %fd255, %fd256;

$L__BB0_19:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd320;
	}
	and.b32  	%r51, %r50, 2147483647;
	setp.eq.s32 	%p22, %r51, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r52, %temp}, %fd320;
	}
	setp.eq.s32 	%p23, %r52, 0;
	and.pred  	%p24, %p23, %p22;
	@%p24 bra 	$L__BB0_21;

	// begin inline asm
	fma.rn.f64 	%fd320, %fd320, %fd11, %fd320;
	// end inline asm

$L__BB0_21:
	setp.neu.f64 	%p25, %fd6, 0d3FF0000000000000;
	or.pred  	%p27, %p15, %p25;
	@%p27 bra 	$L__BB0_28;

	mov.b64 	%rd16, %fd320;
	xor.b64  	%rd17, %rd16, -9223372036854775808;
	mov.b64 	%fd320, %rd17;
	bra.uni 	$L__BB0_28;

}

  